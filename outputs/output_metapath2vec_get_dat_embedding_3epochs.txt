wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbirkc (jbirkc-danmarks-tekniske-universitet-dtu). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /zhome/45/0/155089/.netrc
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /zhome/45/0/155089/deeplearning/Deep-Learning-Project/wandb/run-20241128_072604-e4emk5ko
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-bush-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jbirkc-danmarks-tekniske-universitet-dtu/meta-path2vec
wandb: üöÄ View run at https://wandb.ai/jbirkc-danmarks-tekniske-universitet-dtu/meta-path2vec/runs/e4emk5ko
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Using GPU
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: 1, Step: 00100/8865, Loss: 7.1585
Epoch: 1, Step: 00200/8865, Loss: 6.2963
Epoch: 1, Step: 00300/8865, Loss: 5.5245
Epoch: 1, Step: 00400/8865, Loss: 4.8785
Epoch: 1, Step: 00500/8865, Loss: 4.3643
Epoch: 1, Step: 00600/8865, Loss: 3.9688
Epoch: 1, Step: 00700/8865, Loss: 3.6482
Epoch: 1, Step: 00800/8865, Loss: 3.3730
Epoch: 1, Step: 00900/8865, Loss: 3.1281
Epoch: 1, Step: 01000/8865, Loss: 2.9087
Epoch: 1, Step: 01100/8865, Loss: 2.7130
Epoch: 1, Step: 01200/8865, Loss: 2.5410
Epoch: 1, Step: 01300/8865, Loss: 2.3868
Epoch: 1, Step: 01400/8865, Loss: 2.2548
Epoch: 1, Step: 01500/8865, Loss: 2.1357
Epoch: 1, Step: 01600/8865, Loss: 2.0321
Epoch: 1, Step: 01700/8865, Loss: 1.9406
Epoch: 1, Step: 01800/8865, Loss: 1.8564
Epoch: 1, Step: 01900/8865, Loss: 1.7820
Epoch: 1, Step: 02000/8865, Loss: 1.7182
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 1, Step: 02000/8865, Acc: 0.0352
Epoch: 1, Step: 02100/8865, Loss: 1.6567
Epoch: 1, Step: 02200/8865, Loss: 1.6031
Epoch: 1, Step: 02300/8865, Loss: 1.5525
Epoch: 1, Step: 02400/8865, Loss: 1.5050
Epoch: 1, Step: 02500/8865, Loss: 1.4610
Epoch: 1, Step: 02600/8865, Loss: 1.4206
Epoch: 1, Step: 02700/8865, Loss: 1.3818
Epoch: 1, Step: 02800/8865, Loss: 1.3469
Epoch: 1, Step: 02900/8865, Loss: 1.3131
Epoch: 1, Step: 03000/8865, Loss: 1.2800
Epoch: 1, Step: 03100/8865, Loss: 1.2487
Epoch: 1, Step: 03200/8865, Loss: 1.2186
Epoch: 1, Step: 03300/8865, Loss: 1.1897
Epoch: 1, Step: 03400/8865, Loss: 1.1601
Epoch: 1, Step: 03500/8865, Loss: 1.1330
Epoch: 1, Step: 03600/8865, Loss: 1.1045
Epoch: 1, Step: 03700/8865, Loss: 1.0800
Epoch: 1, Step: 03800/8865, Loss: 1.0539
Epoch: 1, Step: 03900/8865, Loss: 1.0300
Epoch: 1, Step: 04000/8865, Loss: 1.0080
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 1, Step: 04000/8865, Acc: 0.0857
Epoch: 1, Step: 04100/8865, Loss: 0.9871
Epoch: 1, Step: 04200/8865, Loss: 0.9676
Epoch: 1, Step: 04300/8865, Loss: 0.9512
Epoch: 1, Step: 04400/8865, Loss: 0.9364
Epoch: 1, Step: 04500/8865, Loss: 0.9219
Epoch: 1, Step: 04600/8865, Loss: 0.9092
Epoch: 1, Step: 04700/8865, Loss: 0.8984
Epoch: 1, Step: 04800/8865, Loss: 0.8869
Epoch: 1, Step: 04900/8865, Loss: 0.8772
Epoch: 1, Step: 05000/8865, Loss: 0.8674
Epoch: 1, Step: 05100/8865, Loss: 0.8587
Epoch: 1, Step: 05200/8865, Loss: 0.8499
Epoch: 1, Step: 05300/8865, Loss: 0.8409
Epoch: 1, Step: 05400/8865, Loss: 0.8343
Epoch: 1, Step: 05500/8865, Loss: 0.8274
Epoch: 1, Step: 05600/8865, Loss: 0.8203
Epoch: 1, Step: 05700/8865, Loss: 0.8145
Epoch: 1, Step: 05800/8865, Loss: 0.8086
Epoch: 1, Step: 05900/8865, Loss: 0.8032
Epoch: 1, Step: 06000/8865, Loss: 0.7971
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 1, Step: 06000/8865, Acc: 0.1546
Epoch: 1, Step: 06100/8865, Loss: 0.7914
Epoch: 1, Step: 06200/8865, Loss: 0.7865
Epoch: 1, Step: 06300/8865, Loss: 0.7819
Epoch: 1, Step: 06400/8865, Loss: 0.7780
Epoch: 1, Step: 06500/8865, Loss: 0.7728
Epoch: 1, Step: 06600/8865, Loss: 0.7686
Epoch: 1, Step: 06700/8865, Loss: 0.7641
Epoch: 1, Step: 06800/8865, Loss: 0.7605
Epoch: 1, Step: 06900/8865, Loss: 0.7573
Epoch: 1, Step: 07000/8865, Loss: 0.7530
Epoch: 1, Step: 07100/8865, Loss: 0.7499
Epoch: 1, Step: 07200/8865, Loss: 0.7476
Epoch: 1, Step: 07300/8865, Loss: 0.7434
Epoch: 1, Step: 07400/8865, Loss: 0.7401
Epoch: 1, Step: 07500/8865, Loss: 0.7352
Epoch: 1, Step: 07600/8865, Loss: 0.7347
Epoch: 1, Step: 07700/8865, Loss: 0.7315
Epoch: 1, Step: 07800/8865, Loss: 0.7291
Epoch: 1, Step: 07900/8865, Loss: 0.7264
Epoch: 1, Step: 08000/8865, Loss: 0.7241
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 1, Step: 08000/8865, Acc: 0.2203
Epoch: 1, Step: 08100/8865, Loss: 0.7223
Epoch: 1, Step: 08200/8865, Loss: 0.7198
Epoch: 1, Step: 08300/8865, Loss: 0.7164
Epoch: 1, Step: 08400/8865, Loss: 0.7162
Epoch: 1, Step: 08500/8865, Loss: 0.7135
Epoch: 1, Step: 08600/8865, Loss: 0.7113
Epoch: 1, Step: 08700/8865, Loss: 0.7094
Epoch: 1, Step: 08800/8865, Loss: 0.7077
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 1, Accuracy: 0.2362
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: 2, Step: 00100/8865, Loss: 0.7049
Epoch: 2, Step: 00200/8865, Loss: 0.7033
Epoch: 2, Step: 00300/8865, Loss: 0.7021
Epoch: 2, Step: 00400/8865, Loss: 0.7000
Epoch: 2, Step: 00500/8865, Loss: 0.6992
Epoch: 2, Step: 00600/8865, Loss: 0.6974
Epoch: 2, Step: 00700/8865, Loss: 0.6958
Epoch: 2, Step: 00800/8865, Loss: 0.6942
Epoch: 2, Step: 00900/8865, Loss: 0.6945
Epoch: 2, Step: 01000/8865, Loss: 0.6933
Epoch: 2, Step: 01100/8865, Loss: 0.6913
Epoch: 2, Step: 01200/8865, Loss: 0.6897
Epoch: 2, Step: 01300/8865, Loss: 0.6889
Epoch: 2, Step: 01400/8865, Loss: 0.6876
Epoch: 2, Step: 01500/8865, Loss: 0.6876
Epoch: 2, Step: 01600/8865, Loss: 0.6856
Epoch: 2, Step: 01700/8865, Loss: 0.6855
Epoch: 2, Step: 01800/8865, Loss: 0.6842
Epoch: 2, Step: 01900/8865, Loss: 0.6830
Epoch: 2, Step: 02000/8865, Loss: 0.6821
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 2, Step: 02000/8865, Acc: 0.2600
Epoch: 2, Step: 02100/8865, Loss: 0.6822
Epoch: 2, Step: 02200/8865, Loss: 0.6801
Epoch: 2, Step: 02300/8865, Loss: 0.6798
Epoch: 2, Step: 02400/8865, Loss: 0.6788
Epoch: 2, Step: 02500/8865, Loss: 0.6786
Epoch: 2, Step: 02600/8865, Loss: 0.6773
Epoch: 2, Step: 02700/8865, Loss: 0.6767
Epoch: 2, Step: 02800/8865, Loss: 0.6764
Epoch: 2, Step: 02900/8865, Loss: 0.6762
Epoch: 2, Step: 03000/8865, Loss: 0.6749
Epoch: 2, Step: 03100/8865, Loss: 0.6743
Epoch: 2, Step: 03200/8865, Loss: 0.6743
Epoch: 2, Step: 03300/8865, Loss: 0.6731
Epoch: 2, Step: 03400/8865, Loss: 0.6726
Epoch: 2, Step: 03500/8865, Loss: 0.6727
Epoch: 2, Step: 03600/8865, Loss: 0.6721
Epoch: 2, Step: 03700/8865, Loss: 0.6707
Epoch: 2, Step: 03800/8865, Loss: 0.6701
Epoch: 2, Step: 03900/8865, Loss: 0.6710
Epoch: 2, Step: 04000/8865, Loss: 0.6702
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 2, Step: 04000/8865, Acc: 0.2728
Epoch: 2, Step: 04100/8865, Loss: 0.6691
Epoch: 2, Step: 04200/8865, Loss: 0.6702
Epoch: 2, Step: 04300/8865, Loss: 0.6688
Epoch: 2, Step: 04400/8865, Loss: 0.6703
Epoch: 2, Step: 04500/8865, Loss: 0.6697
Epoch: 2, Step: 04600/8865, Loss: 0.6694
Epoch: 2, Step: 04700/8865, Loss: 0.6693
Epoch: 2, Step: 04800/8865, Loss: 0.6683
Epoch: 2, Step: 04900/8865, Loss: 0.6691
Epoch: 2, Step: 05000/8865, Loss: 0.6692
Epoch: 2, Step: 05100/8865, Loss: 0.6690
Epoch: 2, Step: 05200/8865, Loss: 0.6683
Epoch: 2, Step: 05300/8865, Loss: 0.6688
Epoch: 2, Step: 05400/8865, Loss: 0.6699
Epoch: 2, Step: 05500/8865, Loss: 0.6698
Epoch: 2, Step: 05600/8865, Loss: 0.6703
Epoch: 2, Step: 05700/8865, Loss: 0.6700
Epoch: 2, Step: 05800/8865, Loss: 0.6696
Epoch: 2, Step: 05900/8865, Loss: 0.6703
Epoch: 2, Step: 06000/8865, Loss: 0.6699
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 2, Step: 06000/8865, Acc: 0.2820
Epoch: 2, Step: 06100/8865, Loss: 0.6705
Epoch: 2, Step: 06200/8865, Loss: 0.6706
Epoch: 2, Step: 06300/8865, Loss: 0.6715
Epoch: 2, Step: 06400/8865, Loss: 0.6703
Epoch: 2, Step: 06500/8865, Loss: 0.6703
Epoch: 2, Step: 06600/8865, Loss: 0.6697
Epoch: 2, Step: 06700/8865, Loss: 0.6701
Epoch: 2, Step: 06800/8865, Loss: 0.6690
Epoch: 2, Step: 06900/8865, Loss: 0.6694
Epoch: 2, Step: 07000/8865, Loss: 0.6695
Epoch: 2, Step: 07100/8865, Loss: 0.6686
Epoch: 2, Step: 07200/8865, Loss: 0.6676
Epoch: 2, Step: 07300/8865, Loss: 0.6674
Epoch: 2, Step: 07400/8865, Loss: 0.6667
Epoch: 2, Step: 07500/8865, Loss: 0.6658
Epoch: 2, Step: 07600/8865, Loss: 0.6659
Epoch: 2, Step: 07700/8865, Loss: 0.6656
Epoch: 2, Step: 07800/8865, Loss: 0.6654
Epoch: 2, Step: 07900/8865, Loss: 0.6645
Epoch: 2, Step: 08000/8865, Loss: 0.6648
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 2, Step: 08000/8865, Acc: 0.2909
Epoch: 2, Step: 08100/8865, Loss: 0.6635
Epoch: 2, Step: 08200/8865, Loss: 0.6628
Epoch: 2, Step: 08300/8865, Loss: 0.6625
Epoch: 2, Step: 08400/8865, Loss: 0.6623
Epoch: 2, Step: 08500/8865, Loss: 0.6605
Epoch: 2, Step: 08600/8865, Loss: 0.6601
Epoch: 2, Step: 08700/8865, Loss: 0.6601
Epoch: 2, Step: 08800/8865, Loss: 0.6588
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 2, Accuracy: 0.2941
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: 3, Step: 00100/8865, Loss: 0.6580
Epoch: 3, Step: 00200/8865, Loss: 0.6568
Epoch: 3, Step: 00300/8865, Loss: 0.6565
Epoch: 3, Step: 00400/8865, Loss: 0.6560
Epoch: 3, Step: 00500/8865, Loss: 0.6553
Epoch: 3, Step: 00600/8865, Loss: 0.6555
Epoch: 3, Step: 00700/8865, Loss: 0.6549
Epoch: 3, Step: 00800/8865, Loss: 0.6545
Epoch: 3, Step: 00900/8865, Loss: 0.6543
Epoch: 3, Step: 01000/8865, Loss: 0.6535
Epoch: 3, Step: 01100/8865, Loss: 0.6531
Epoch: 3, Step: 01200/8865, Loss: 0.6526
Epoch: 3, Step: 01300/8865, Loss: 0.6527
Epoch: 3, Step: 01400/8865, Loss: 0.6520
Epoch: 3, Step: 01500/8865, Loss: 0.6522
Epoch: 3, Step: 01600/8865, Loss: 0.6514
Epoch: 3, Step: 01700/8865, Loss: 0.6516
Epoch: 3, Step: 01800/8865, Loss: 0.6506
Epoch: 3, Step: 01900/8865, Loss: 0.6508
Epoch: 3, Step: 02000/8865, Loss: 0.6500
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 3, Step: 02000/8865, Acc: 0.2962
Epoch: 3, Step: 02100/8865, Loss: 0.6503
Epoch: 3, Step: 02200/8865, Loss: 0.6502
Epoch: 3, Step: 02300/8865, Loss: 0.6491
Epoch: 3, Step: 02400/8865, Loss: 0.6489
Epoch: 3, Step: 02500/8865, Loss: 0.6501
Epoch: 3, Step: 02600/8865, Loss: 0.6500
Epoch: 3, Step: 02700/8865, Loss: 0.6495
Epoch: 3, Step: 02800/8865, Loss: 0.6487
Epoch: 3, Step: 02900/8865, Loss: 0.6478
Epoch: 3, Step: 03000/8865, Loss: 0.6479
Epoch: 3, Step: 03100/8865, Loss: 0.6477
Epoch: 3, Step: 03200/8865, Loss: 0.6476
Epoch: 3, Step: 03300/8865, Loss: 0.6470
Epoch: 3, Step: 03400/8865, Loss: 0.6475
Epoch: 3, Step: 03500/8865, Loss: 0.6478
Epoch: 3, Step: 03600/8865, Loss: 0.6471
Epoch: 3, Step: 03700/8865, Loss: 0.6469
Epoch: 3, Step: 03800/8865, Loss: 0.6475
Epoch: 3, Step: 03900/8865, Loss: 0.6471
Epoch: 3, Step: 04000/8865, Loss: 0.6462
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 3, Step: 04000/8865, Acc: 0.2986
Epoch: 3, Step: 04100/8865, Loss: 0.6467
Epoch: 3, Step: 04200/8865, Loss: 0.6458
Epoch: 3, Step: 04300/8865, Loss: 0.6467
Epoch: 3, Step: 04400/8865, Loss: 0.6460
Epoch: 3, Step: 04500/8865, Loss: 0.6467
Epoch: 3, Step: 04600/8865, Loss: 0.6466
Epoch: 3, Step: 04700/8865, Loss: 0.6458
Epoch: 3, Step: 04800/8865, Loss: 0.6451
Epoch: 3, Step: 04900/8865, Loss: 0.6454
Epoch: 3, Step: 05000/8865, Loss: 0.6445
Epoch: 3, Step: 05100/8865, Loss: 0.6453
Epoch: 3, Step: 05200/8865, Loss: 0.6450
Epoch: 3, Step: 05300/8865, Loss: 0.6454
Epoch: 3, Step: 05400/8865, Loss: 0.6459
Epoch: 3, Step: 05500/8865, Loss: 0.6451
Epoch: 3, Step: 05600/8865, Loss: 0.6443
Epoch: 3, Step: 05700/8865, Loss: 0.6454
Epoch: 3, Step: 05800/8865, Loss: 0.6447
Epoch: 3, Step: 05900/8865, Loss: 0.6443
Epoch: 3, Step: 06000/8865, Loss: 0.6439
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 3, Step: 06000/8865, Acc: 0.3028
Epoch: 3, Step: 06100/8865, Loss: 0.6441
Epoch: 3, Step: 06200/8865, Loss: 0.6445
Epoch: 3, Step: 06300/8865, Loss: 0.6437
Epoch: 3, Step: 06400/8865, Loss: 0.6441
Epoch: 3, Step: 06500/8865, Loss: 0.6441
Epoch: 3, Step: 06600/8865, Loss: 0.6433
Epoch: 3, Step: 06700/8865, Loss: 0.6443
Epoch: 3, Step: 06800/8865, Loss: 0.6438
Epoch: 3, Step: 06900/8865, Loss: 0.6439
Epoch: 3, Step: 07000/8865, Loss: 0.6434
Epoch: 3, Step: 07100/8865, Loss: 0.6439
Epoch: 3, Step: 07200/8865, Loss: 0.6431
Epoch: 3, Step: 07300/8865, Loss: 0.6425
Epoch: 3, Step: 07400/8865, Loss: 0.6426
Epoch: 3, Step: 07500/8865, Loss: 0.6428
Epoch: 3, Step: 07600/8865, Loss: 0.6432
Epoch: 3, Step: 07700/8865, Loss: 0.6423
Epoch: 3, Step: 07800/8865, Loss: 0.6417
Epoch: 3, Step: 07900/8865, Loss: 0.6427
Epoch: 3, Step: 08000/8865, Loss: 0.6431
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 3, Step: 08000/8865, Acc: 0.3043
Epoch: 3, Step: 08100/8865, Loss: 0.6415
Epoch: 3, Step: 08200/8865, Loss: 0.6422
Epoch: 3, Step: 08300/8865, Loss: 0.6411
Epoch: 3, Step: 08400/8865, Loss: 0.6410
Epoch: 3, Step: 08500/8865, Loss: 0.6412
Epoch: 3, Step: 08600/8865, Loss: 0.6420
Epoch: 3, Step: 08700/8865, Loss: 0.6413
Epoch: 3, Step: 08800/8865, Loss: 0.6415
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 3, Accuracy: 0.3062
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
