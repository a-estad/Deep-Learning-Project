{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import MetaPath2Vec\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/a6/2/155003/deep_learning/final_project/project_venv/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(\n",
       "  num_nodes_dict={\n",
       "    author=1134649,\n",
       "    field_of_study=59965,\n",
       "    institution=8740,\n",
       "    paper=736389,\n",
       "  },\n",
       "  edge_index_dict={\n",
       "    (author, affiliated_with, institution)=[2, 1043998],\n",
       "    (author, writes, paper)=[2, 7145660],\n",
       "    (paper, cites, paper)=[2, 5416271],\n",
       "    (paper, has_topic, field_of_study)=[2, 7505078],\n",
       "  },\n",
       "  x_dict={ paper=[736389, 128] },\n",
       "  node_year={ paper=[736389, 1] },\n",
       "  edge_reltype={\n",
       "    (author, affiliated_with, institution)=[1043998, 1],\n",
       "    (author, writes, paper)=[7145660, 1],\n",
       "    (paper, cites, paper)=[5416271, 1],\n",
       "    (paper, has_topic, field_of_study)=[7505078, 1],\n",
       "  },\n",
       "  y_dict={ paper=[736389, 1] }\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = PygNodePropPredDataset(name = \"ogbn-mag\")\n",
    "\n",
    "# split_idx = dataset.get_idx_split()\n",
    "# train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "# train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True)\n",
    "# valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False)\n",
    "# test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False)\n",
    "\n",
    "graph = dataset[0]\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('author',\n",
       "  'writes',\n",
       "  'paper'): tensor([[      0,       0,       0,  ..., 1134647, 1134648, 1134648],\n",
       "         [  19703,  289285,  311768,  ...,  657395,  671118,  719594]]),\n",
       " ('paper',\n",
       "  'cites',\n",
       "  'paper'): tensor([[     0,      0,      0,  ..., 736388, 736388, 736388],\n",
       "         [    88,  27449, 121051,  ..., 421711, 427339, 439864]]),\n",
       " ('paper',\n",
       "  'has_topic',\n",
       "  'field_of_study'): tensor([[     0,      0,      0,  ..., 736388, 736388, 736388],\n",
       "         [   145,   2215,   3205,  ...,  21458,  22283,  31934]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del graph.edge_index_dict[('author', 'affiliated_with', 'institution')]\n",
    "graph.edge_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step: 00100/8865, Loss: 9.9194\n",
      "Epoch: 1, Step: 00200/8865, Loss: 9.7962\n",
      "Epoch: 1, Step: 00300/8865, Loss: 9.5744\n",
      "Epoch: 1, Step: 00400/8865, Loss: 9.3742\n",
      "Epoch: 1, Step: 00500/8865, Loss: 9.2839\n",
      "Epoch: 1, Step: 00600/8865, Loss: 9.1937\n",
      "Epoch: 1, Step: 00700/8865, Loss: 9.1473\n",
      "Epoch: 1, Step: 00800/8865, Loss: 9.1229\n",
      "Epoch: 1, Step: 00900/8865, Loss: 9.1325\n",
      "Epoch: 1, Step: 01000/8865, Loss: 9.0791\n",
      "Epoch: 1, Step: 01100/8865, Loss: 9.0311\n",
      "Epoch: 1, Step: 01200/8865, Loss: 9.0386\n",
      "Epoch: 1, Step: 01300/8865, Loss: 8.9932\n",
      "Epoch: 1, Step: 01400/8865, Loss: 8.9473\n",
      "Epoch: 1, Step: 01500/8865, Loss: 8.9716\n",
      "Epoch: 1, Step: 01600/8865, Loss: 8.8883\n",
      "Epoch: 1, Step: 01700/8865, Loss: 8.9272\n",
      "Epoch: 1, Step: 01800/8865, Loss: 8.8569\n",
      "Epoch: 1, Step: 01900/8865, Loss: 8.8382\n",
      "Epoch: 1, Step: 02000/8865, Loss: 8.8433\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 59\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtest(z[train_perm], y[train_perm], z[test_perm], y[test_perm],\n\u001b[1;32m     55\u001b[0m                       max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     acc \u001b[38;5;241m=\u001b[39m test()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, log_steps, eval_steps)\u001b[0m\n\u001b[1;32m     35\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m eval_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/deep_learning/final_project/project_venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 47\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(train_ratio)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(train_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[1;32m     45\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 47\u001b[0m     z \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper\u001b[39m\u001b[38;5;124m'\u001b[39m, batch\u001b[38;5;241m=\u001b[39m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device))\n\u001b[1;32m     48\u001b[0m     y \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39my_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     50\u001b[0m     perm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(z\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "metapath = [\n",
    "    ('author', 'writes', 'paper'),\n",
    "    ('paper', 'cites', 'paper'),\n",
    "    ('paper', 'has_topic', 'field_of_study'),\n",
    "    # ('author', 'affiliated_with', 'institution'),\n",
    "]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "model = MetaPath2Vec(graph.edge_index_dict, embedding_dim=128,\n",
    "                     metapath=metapath, walk_length=3, context_size=4,\n",
    "                     walks_per_node=5, num_negative_samples=5,\n",
    "                     sparse=True).to(device)\n",
    "\n",
    "\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=6)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "\n",
    "def train(epoch, log_steps=100, eval_steps=2000):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % log_steps == 0:\n",
    "            print(f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                  f'Loss: {total_loss / log_steps:.4f}')\n",
    "            total_loss = 0\n",
    "\n",
    "        if (i + 1) % eval_steps == 0:\n",
    "            acc = test()\n",
    "            print(f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                  f'Acc: {acc:.4f}')\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(train_ratio=0.1):\n",
    "    model.eval()\n",
    "\n",
    "    z = model('paper', batch=graph.x_dict['paper'].to(device))\n",
    "    y = graph.y_dict['paper']\n",
    "\n",
    "    perm = torch.randperm(z.size(0))\n",
    "    train_perm = perm[:int(z.size(0) * train_ratio)]\n",
    "    test_perm = perm[int(z.size(0) * train_ratio):]\n",
    "\n",
    "    return model.test(z[train_perm], y[train_perm], z[test_perm], y[test_perm],\n",
    "                      max_iter=150)\n",
    "\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(epoch)\n",
    "    acc = test()\n",
    "    print(f'Epoch: {epoch}, Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0954,  0.0408, -0.2109,  ...,  0.0616, -0.0277, -0.1338],\n",
       "        [-0.1510, -0.1073, -0.2220,  ...,  0.3458, -0.0277, -0.2185],\n",
       "        [-0.1148, -0.1760, -0.2606,  ...,  0.1731, -0.1564, -0.2780],\n",
       "        ...,\n",
       "        [ 0.0228, -0.0865,  0.0981,  ..., -0.0547, -0.2077, -0.2305],\n",
       "        [-0.2891, -0.2029, -0.1525,  ...,  0.1042,  0.2041, -0.3528],\n",
       "        [-0.0890, -0.0348, -0.2642,  ...,  0.2601, -0.0875, -0.5171]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.x_dict['paper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('author', 'affiliated_with', 'institution'),\n",
       "  ('author', 'writes', 'paper'),\n",
       "  ('paper', 'cites', 'paper')],\n",
       " [('author', 'writes', 'paper'),\n",
       "  ('paper', 'cites', 'paper'),\n",
       "  ('paper', 'has_topic', 'field_of_study')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metapath[:-1], metapath[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "institution author\n",
      "paper paper\n",
      "paper paper\n"
     ]
    }
   ],
   "source": [
    "for edge_type1, edge_type2 in zip(metapath[:-1], metapath[1:]):\n",
    "    # if edge_type1[-1] != edge_type2[0]:\n",
    "    print(edge_type1[-1], edge_type2[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
