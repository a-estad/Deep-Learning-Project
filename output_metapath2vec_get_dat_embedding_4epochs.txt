wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbirkc (jbirkc-danmarks-tekniske-universitet-dtu). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /zhome/45/0/155089/.netrc
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /zhome/45/0/155089/deeplearning/Deep-Learning-Project/wandb/run-20241129_125517-pgblzq13
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-valley-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jbirkc-danmarks-tekniske-universitet-dtu/meta-path2vec
wandb: üöÄ View run at https://wandb.ai/jbirkc-danmarks-tekniske-universitet-dtu/meta-path2vec/runs/pgblzq13
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Using GPU
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: 1, Step: 00100/8865, Loss: 7.1666
Epoch: 1, Step: 00200/8865, Loss: 6.3007
Epoch: 1, Step: 00300/8865, Loss: 5.5253
Epoch: 1, Step: 00400/8865, Loss: 4.8819
Epoch: 1, Step: 00500/8865, Loss: 4.3668
Epoch: 1, Step: 00600/8865, Loss: 3.9625
Epoch: 1, Step: 00700/8865, Loss: 3.6458
Epoch: 1, Step: 00800/8865, Loss: 3.3702
Epoch: 1, Step: 00900/8865, Loss: 3.1251
Epoch: 1, Step: 01000/8865, Loss: 2.9037
Epoch: 1, Step: 01100/8865, Loss: 2.7100
Epoch: 1, Step: 01200/8865, Loss: 2.5404
Epoch: 1, Step: 01300/8865, Loss: 2.3839
Epoch: 1, Step: 01400/8865, Loss: 2.2499
Epoch: 1, Step: 01500/8865, Loss: 2.1331
Epoch: 1, Step: 01600/8865, Loss: 2.0317
Epoch: 1, Step: 01700/8865, Loss: 1.9388
Epoch: 1, Step: 01800/8865, Loss: 1.8589
Epoch: 1, Step: 01900/8865, Loss: 1.7844
Epoch: 1, Step: 02000/8865, Loss: 1.7165
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 1, Step: 02000/8865, Acc: 0.0367
Epoch: 1, Step: 02100/8865, Loss: 1.6572
Epoch: 1, Step: 02200/8865, Loss: 1.6014
Epoch: 1, Step: 02300/8865, Loss: 1.5502
Epoch: 1, Step: 02400/8865, Loss: 1.5040
Epoch: 1, Step: 02500/8865, Loss: 1.4615
Epoch: 1, Step: 02600/8865, Loss: 1.4195
Epoch: 1, Step: 02700/8865, Loss: 1.3827
Epoch: 1, Step: 02800/8865, Loss: 1.3461
Epoch: 1, Step: 02900/8865, Loss: 1.3126
Epoch: 1, Step: 03000/8865, Loss: 1.2789
Epoch: 1, Step: 03100/8865, Loss: 1.2477
Epoch: 1, Step: 03200/8865, Loss: 1.2180
Epoch: 1, Step: 03300/8865, Loss: 1.1896
Epoch: 1, Step: 03400/8865, Loss: 1.1605
Epoch: 1, Step: 03500/8865, Loss: 1.1331
Epoch: 1, Step: 03600/8865, Loss: 1.1074
Epoch: 1, Step: 03700/8865, Loss: 1.0803
Epoch: 1, Step: 03800/8865, Loss: 1.0548
Epoch: 1, Step: 03900/8865, Loss: 1.0301
Epoch: 1, Step: 04000/8865, Loss: 1.0080
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 1, Step: 04000/8865, Acc: 0.0886
Epoch: 1, Step: 04100/8865, Loss: 0.9868
Epoch: 1, Step: 04200/8865, Loss: 0.9683
Epoch: 1, Step: 04300/8865, Loss: 0.9523
Epoch: 1, Step: 04400/8865, Loss: 0.9378
Epoch: 1, Step: 04500/8865, Loss: 0.9223
Epoch: 1, Step: 04600/8865, Loss: 0.9107
Epoch: 1, Step: 04700/8865, Loss: 0.8992
Epoch: 1, Step: 04800/8865, Loss: 0.8874
Epoch: 1, Step: 04900/8865, Loss: 0.8781
Epoch: 1, Step: 05000/8865, Loss: 0.8686
Epoch: 1, Step: 05100/8865, Loss: 0.8587
Epoch: 1, Step: 05200/8865, Loss: 0.8500
Epoch: 1, Step: 05300/8865, Loss: 0.8428
Epoch: 1, Step: 05400/8865, Loss: 0.8352
Epoch: 1, Step: 05500/8865, Loss: 0.8276
Epoch: 1, Step: 05600/8865, Loss: 0.8201
Epoch: 1, Step: 05700/8865, Loss: 0.8150
Epoch: 1, Step: 05800/8865, Loss: 0.8085
Epoch: 1, Step: 05900/8865, Loss: 0.8025
Epoch: 1, Step: 06000/8865, Loss: 0.7965
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 1, Step: 06000/8865, Acc: 0.1544
Epoch: 1, Step: 06100/8865, Loss: 0.7926
Epoch: 1, Step: 06200/8865, Loss: 0.7870
Epoch: 1, Step: 06300/8865, Loss: 0.7824
Epoch: 1, Step: 06400/8865, Loss: 0.7763
Epoch: 1, Step: 06500/8865, Loss: 0.7736
Epoch: 1, Step: 06600/8865, Loss: 0.7687
Epoch: 1, Step: 06700/8865, Loss: 0.7636
Epoch: 1, Step: 06800/8865, Loss: 0.7603
Epoch: 1, Step: 06900/8865, Loss: 0.7571
Epoch: 1, Step: 07000/8865, Loss: 0.7537
Epoch: 1, Step: 07100/8865, Loss: 0.7504
Epoch: 1, Step: 07200/8865, Loss: 0.7472
Epoch: 1, Step: 07300/8865, Loss: 0.7438
Epoch: 1, Step: 07400/8865, Loss: 0.7396
Epoch: 1, Step: 07500/8865, Loss: 0.7373
Epoch: 1, Step: 07600/8865, Loss: 0.7337
Epoch: 1, Step: 07700/8865, Loss: 0.7318
Epoch: 1, Step: 07800/8865, Loss: 0.7296
Epoch: 1, Step: 07900/8865, Loss: 0.7269
Epoch: 1, Step: 08000/8865, Loss: 0.7234
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 1, Step: 08000/8865, Acc: 0.2158
Epoch: 1, Step: 08100/8865, Loss: 0.7219
Epoch: 1, Step: 08200/8865, Loss: 0.7196
Epoch: 1, Step: 08300/8865, Loss: 0.7166
Epoch: 1, Step: 08400/8865, Loss: 0.7161
Epoch: 1, Step: 08500/8865, Loss: 0.7135
Epoch: 1, Step: 08600/8865, Loss: 0.7116
Epoch: 1, Step: 08700/8865, Loss: 0.7092
Epoch: 1, Step: 08800/8865, Loss: 0.7084
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 1, Accuracy: 0.2348
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: 2, Step: 00100/8865, Loss: 0.7042
Epoch: 2, Step: 00200/8865, Loss: 0.7036
Epoch: 2, Step: 00300/8865, Loss: 0.7018
Epoch: 2, Step: 00400/8865, Loss: 0.7002
Epoch: 2, Step: 00500/8865, Loss: 0.6977
Epoch: 2, Step: 00600/8865, Loss: 0.6978
Epoch: 2, Step: 00700/8865, Loss: 0.6967
Epoch: 2, Step: 00800/8865, Loss: 0.6935
Epoch: 2, Step: 00900/8865, Loss: 0.6935
Epoch: 2, Step: 01000/8865, Loss: 0.6925
Epoch: 2, Step: 01100/8865, Loss: 0.6906
Epoch: 2, Step: 01200/8865, Loss: 0.6898
Epoch: 2, Step: 01300/8865, Loss: 0.6893
Epoch: 2, Step: 01400/8865, Loss: 0.6871
Epoch: 2, Step: 01500/8865, Loss: 0.6868
Epoch: 2, Step: 01600/8865, Loss: 0.6851
Epoch: 2, Step: 01700/8865, Loss: 0.6850
Epoch: 2, Step: 01800/8865, Loss: 0.6847
Epoch: 2, Step: 01900/8865, Loss: 0.6826
Epoch: 2, Step: 02000/8865, Loss: 0.6817
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 2, Step: 02000/8865, Acc: 0.2595
Epoch: 2, Step: 02100/8865, Loss: 0.6812
Epoch: 2, Step: 02200/8865, Loss: 0.6811
Epoch: 2, Step: 02300/8865, Loss: 0.6799
Epoch: 2, Step: 02400/8865, Loss: 0.6803
Epoch: 2, Step: 02500/8865, Loss: 0.6783
Epoch: 2, Step: 02600/8865, Loss: 0.6781
Epoch: 2, Step: 02700/8865, Loss: 0.6777
Epoch: 2, Step: 02800/8865, Loss: 0.6750
Epoch: 2, Step: 02900/8865, Loss: 0.6748
Epoch: 2, Step: 03000/8865, Loss: 0.6749
Epoch: 2, Step: 03100/8865, Loss: 0.6742
Epoch: 2, Step: 03200/8865, Loss: 0.6734
Epoch: 2, Step: 03300/8865, Loss: 0.6737
Epoch: 2, Step: 03400/8865, Loss: 0.6722
Epoch: 2, Step: 03500/8865, Loss: 0.6709
Epoch: 2, Step: 03600/8865, Loss: 0.6718
Epoch: 2, Step: 03700/8865, Loss: 0.6714
Epoch: 2, Step: 03800/8865, Loss: 0.6708
Epoch: 2, Step: 03900/8865, Loss: 0.6696
Epoch: 2, Step: 04000/8865, Loss: 0.6698
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 2, Step: 04000/8865, Acc: 0.2738
Epoch: 2, Step: 04100/8865, Loss: 0.6694
Epoch: 2, Step: 04200/8865, Loss: 0.6694
Epoch: 2, Step: 04300/8865, Loss: 0.6687
Epoch: 2, Step: 04400/8865, Loss: 0.6684
Epoch: 2, Step: 04500/8865, Loss: 0.6683
Epoch: 2, Step: 04600/8865, Loss: 0.6687
Epoch: 2, Step: 04700/8865, Loss: 0.6680
Epoch: 2, Step: 04800/8865, Loss: 0.6675
Epoch: 2, Step: 04900/8865, Loss: 0.6676
Epoch: 2, Step: 05000/8865, Loss: 0.6670
Epoch: 2, Step: 05100/8865, Loss: 0.6671
Epoch: 2, Step: 05200/8865, Loss: 0.6683
Epoch: 2, Step: 05300/8865, Loss: 0.6680
Epoch: 2, Step: 05400/8865, Loss: 0.6686
Epoch: 2, Step: 05500/8865, Loss: 0.6692
Epoch: 2, Step: 05600/8865, Loss: 0.6686
Epoch: 2, Step: 05700/8865, Loss: 0.6688
Epoch: 2, Step: 05800/8865, Loss: 0.6695
Epoch: 2, Step: 05900/8865, Loss: 0.6683
Epoch: 2, Step: 06000/8865, Loss: 0.6684
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 2, Step: 06000/8865, Acc: 0.2823
Epoch: 2, Step: 06100/8865, Loss: 0.6694
Epoch: 2, Step: 06200/8865, Loss: 0.6691
Epoch: 2, Step: 06300/8865, Loss: 0.6690
Epoch: 2, Step: 06400/8865, Loss: 0.6688
Epoch: 2, Step: 06500/8865, Loss: 0.6691
Epoch: 2, Step: 06600/8865, Loss: 0.6698
Epoch: 2, Step: 06700/8865, Loss: 0.6696
Epoch: 2, Step: 06800/8865, Loss: 0.6690
Epoch: 2, Step: 06900/8865, Loss: 0.6694
Epoch: 2, Step: 07000/8865, Loss: 0.6688
Epoch: 2, Step: 07100/8865, Loss: 0.6683
Epoch: 2, Step: 07200/8865, Loss: 0.6686
Epoch: 2, Step: 07300/8865, Loss: 0.6678
Epoch: 2, Step: 07400/8865, Loss: 0.6676
Epoch: 2, Step: 07500/8865, Loss: 0.6675
Epoch: 2, Step: 07600/8865, Loss: 0.6670
Epoch: 2, Step: 07700/8865, Loss: 0.6664
Epoch: 2, Step: 07800/8865, Loss: 0.6656
Epoch: 2, Step: 07900/8865, Loss: 0.6646
Epoch: 2, Step: 08000/8865, Loss: 0.6636
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 2, Step: 08000/8865, Acc: 0.2884
Epoch: 2, Step: 08100/8865, Loss: 0.6635
Epoch: 2, Step: 08200/8865, Loss: 0.6635
Epoch: 2, Step: 08300/8865, Loss: 0.6631
Epoch: 2, Step: 08400/8865, Loss: 0.6622
Epoch: 2, Step: 08500/8865, Loss: 0.6614
Epoch: 2, Step: 08600/8865, Loss: 0.6613
Epoch: 2, Step: 08700/8865, Loss: 0.6598
Epoch: 2, Step: 08800/8865, Loss: 0.6593
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 2, Accuracy: 0.2911
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Epoch: 3, Step: 00100/8865, Loss: 0.6586
Epoch: 3, Step: 00200/8865, Loss: 0.6584
Epoch: 3, Step: 00300/8865, Loss: 0.6576
Epoch: 3, Step: 00400/8865, Loss: 0.6569
Epoch: 3, Step: 00500/8865, Loss: 0.6564
Epoch: 3, Step: 00600/8865, Loss: 0.6560
Epoch: 3, Step: 00700/8865, Loss: 0.6553
Epoch: 3, Step: 00800/8865, Loss: 0.6547
Epoch: 3, Step: 00900/8865, Loss: 0.6551
Epoch: 3, Step: 01000/8865, Loss: 0.6543
Epoch: 3, Step: 01100/8865, Loss: 0.6526
Epoch: 3, Step: 01200/8865, Loss: 0.6543
Epoch: 3, Step: 01300/8865, Loss: 0.6534
Epoch: 3, Step: 01400/8865, Loss: 0.6523
Epoch: 3, Step: 01500/8865, Loss: 0.6518
Epoch: 3, Step: 01600/8865, Loss: 0.6520
Epoch: 3, Step: 01700/8865, Loss: 0.6511
Epoch: 3, Step: 01800/8865, Loss: 0.6517
Epoch: 3, Step: 01900/8865, Loss: 0.6512
Epoch: 3, Step: 02000/8865, Loss: 0.6506
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 3, Step: 02000/8865, Acc: 0.2957
Epoch: 3, Step: 02100/8865, Loss: 0.6509
Epoch: 3, Step: 02200/8865, Loss: 0.6505
Epoch: 3, Step: 02300/8865, Loss: 0.6497
Epoch: 3, Step: 02400/8865, Loss: 0.6494
Epoch: 3, Step: 02500/8865, Loss: 0.6488
Epoch: 3, Step: 02600/8865, Loss: 0.6489
Epoch: 3, Step: 02700/8865, Loss: 0.6490
Epoch: 3, Step: 02800/8865, Loss: 0.6480
Epoch: 3, Step: 02900/8865, Loss: 0.6491
Epoch: 3, Step: 03000/8865, Loss: 0.6492
Epoch: 3, Step: 03100/8865, Loss: 0.6476
Epoch: 3, Step: 03200/8865, Loss: 0.6487
Epoch: 3, Step: 03300/8865, Loss: 0.6486
Epoch: 3, Step: 03400/8865, Loss: 0.6478
Epoch: 3, Step: 03500/8865, Loss: 0.6477
Epoch: 3, Step: 03600/8865, Loss: 0.6479
Epoch: 3, Step: 03700/8865, Loss: 0.6473
Epoch: 3, Step: 03800/8865, Loss: 0.6475
Epoch: 3, Step: 03900/8865, Loss: 0.6474
Epoch: 3, Step: 04000/8865, Loss: 0.6465
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 3, Step: 04000/8865, Acc: 0.2989
Epoch: 3, Step: 04100/8865, Loss: 0.6465
Epoch: 3, Step: 04200/8865, Loss: 0.6464
Epoch: 3, Step: 04300/8865, Loss: 0.6462
Epoch: 3, Step: 04400/8865, Loss: 0.6463
Epoch: 3, Step: 04500/8865, Loss: 0.6461
Epoch: 3, Step: 04600/8865, Loss: 0.6460
Epoch: 3, Step: 04700/8865, Loss: 0.6464
Epoch: 3, Step: 04800/8865, Loss: 0.6457
Epoch: 3, Step: 04900/8865, Loss: 0.6450
Epoch: 3, Step: 05000/8865, Loss: 0.6454
Epoch: 3, Step: 05100/8865, Loss: 0.6457
Epoch: 3, Step: 05200/8865, Loss: 0.6454
Epoch: 3, Step: 05300/8865, Loss: 0.6458
Epoch: 3, Step: 05400/8865, Loss: 0.6454
Epoch: 3, Step: 05500/8865, Loss: 0.6448
Epoch: 3, Step: 05600/8865, Loss: 0.6456
Epoch: 3, Step: 05700/8865, Loss: 0.6441
Epoch: 3, Step: 05800/8865, Loss: 0.6451
Epoch: 3, Step: 05900/8865, Loss: 0.6438
Epoch: 3, Step: 06000/8865, Loss: 0.6442
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 3, Step: 06000/8865, Acc: 0.3018
Epoch: 3, Step: 06100/8865, Loss: 0.6431
Epoch: 3, Step: 06200/8865, Loss: 0.6439
Epoch: 3, Step: 06300/8865, Loss: 0.6429
Epoch: 3, Step: 06400/8865, Loss: 0.6438
Epoch: 3, Step: 06500/8865, Loss: 0.6433
Epoch: 3, Step: 06600/8865, Loss: 0.6433
Epoch: 3, Step: 06700/8865, Loss: 0.6427
Epoch: 3, Step: 06800/8865, Loss: 0.6433
Epoch: 3, Step: 06900/8865, Loss: 0.6428
Epoch: 3, Step: 07000/8865, Loss: 0.6426
Epoch: 3, Step: 07100/8865, Loss: 0.6428
Epoch: 3, Step: 07200/8865, Loss: 0.6427
Epoch: 3, Step: 07300/8865, Loss: 0.6424
Epoch: 3, Step: 07400/8865, Loss: 0.6426
Epoch: 3, Step: 07500/8865, Loss: 0.6425
Epoch: 3, Step: 07600/8865, Loss: 0.6425
Epoch: 3, Step: 07700/8865, Loss: 0.6419
Epoch: 3, Step: 07800/8865, Loss: 0.6420
Epoch: 3, Step: 07900/8865, Loss: 0.6416
Epoch: 3, Step: 08000/8865, Loss: 0.6420
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 3, Step: 08000/8865, Acc: 0.3017
Epoch: 3, Step: 08100/8865, Loss: 0.6411
Epoch: 3, Step: 08200/8865, Loss: 0.6410
Epoch: 3, Step: 08300/8865, Loss: 0.6409
Epoch: 3, Step: 08400/8865, Loss: 0.6410
Epoch: 3, Step: 08500/8865, Loss: 0.6396
Epoch: 3, Step: 08600/8865, Loss: 0.6411
Epoch: 3, Step: 08700/8865, Loss: 0.6409
Epoch: 3, Step: 08800/8865, Loss: 0.6399
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 3, Accuracy: 0.3037
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
/zhome/45/0/155089/deeplearning/venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
wandb: - 0.056 MB of 0.056 MB uploadedwandb: \ 0.057 MB of 0.057 MB uploadedwandb: | 0.057 MB of 0.057 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: accuracy ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:     loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:     step ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñà
wandb: 
wandb: Run summary:
wandb: accuracy 0.30997
wandb:    epoch 4
wandb:     loss 0.62936
wandb:     step 8800
wandb: 
wandb: üöÄ View run zany-valley-8 at: https://wandb.ai/jbirkc-danmarks-tekniske-universitet-dtu/meta-path2vec/runs/pgblzq13
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jbirkc-danmarks-tekniske-universitet-dtu/meta-path2vec
wandb: Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_125517-pgblzq13/logs
Epoch: 4, Step: 00100/8865, Loss: 0.6403
Epoch: 4, Step: 00200/8865, Loss: 0.6391
Epoch: 4, Step: 00300/8865, Loss: 0.6391
Epoch: 4, Step: 00400/8865, Loss: 0.6391
Epoch: 4, Step: 00500/8865, Loss: 0.6385
Epoch: 4, Step: 00600/8865, Loss: 0.6394
Epoch: 4, Step: 00700/8865, Loss: 0.6382
Epoch: 4, Step: 00800/8865, Loss: 0.6381
Epoch: 4, Step: 00900/8865, Loss: 0.6375
Epoch: 4, Step: 01000/8865, Loss: 0.6377
Epoch: 4, Step: 01100/8865, Loss: 0.6377
Epoch: 4, Step: 01200/8865, Loss: 0.6370
Epoch: 4, Step: 01300/8865, Loss: 0.6376
Epoch: 4, Step: 01400/8865, Loss: 0.6367
Epoch: 4, Step: 01500/8865, Loss: 0.6377
Epoch: 4, Step: 01600/8865, Loss: 0.6366
Epoch: 4, Step: 01700/8865, Loss: 0.6373
Epoch: 4, Step: 01800/8865, Loss: 0.6365
Epoch: 4, Step: 01900/8865, Loss: 0.6365
Epoch: 4, Step: 02000/8865, Loss: 0.6362
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 4, Step: 02000/8865, Acc: 0.3059
Epoch: 4, Step: 02100/8865, Loss: 0.6365
Epoch: 4, Step: 02200/8865, Loss: 0.6357
Epoch: 4, Step: 02300/8865, Loss: 0.6360
Epoch: 4, Step: 02400/8865, Loss: 0.6359
Epoch: 4, Step: 02500/8865, Loss: 0.6359
Epoch: 4, Step: 02600/8865, Loss: 0.6358
Epoch: 4, Step: 02700/8865, Loss: 0.6354
Epoch: 4, Step: 02800/8865, Loss: 0.6353
Epoch: 4, Step: 02900/8865, Loss: 0.6354
Epoch: 4, Step: 03000/8865, Loss: 0.6366
Epoch: 4, Step: 03100/8865, Loss: 0.6348
Epoch: 4, Step: 03200/8865, Loss: 0.6350
Epoch: 4, Step: 03300/8865, Loss: 0.6359
Epoch: 4, Step: 03400/8865, Loss: 0.6358
Epoch: 4, Step: 03500/8865, Loss: 0.6344
Epoch: 4, Step: 03600/8865, Loss: 0.6344
Epoch: 4, Step: 03700/8865, Loss: 0.6342
Epoch: 4, Step: 03800/8865, Loss: 0.6346
Epoch: 4, Step: 03900/8865, Loss: 0.6335
Epoch: 4, Step: 04000/8865, Loss: 0.6345
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 4, Step: 04000/8865, Acc: 0.3035
Epoch: 4, Step: 04100/8865, Loss: 0.6349
Epoch: 4, Step: 04200/8865, Loss: 0.6341
Epoch: 4, Step: 04300/8865, Loss: 0.6344
Epoch: 4, Step: 04400/8865, Loss: 0.6331
Epoch: 4, Step: 04500/8865, Loss: 0.6340
Epoch: 4, Step: 04600/8865, Loss: 0.6326
Epoch: 4, Step: 04700/8865, Loss: 0.6335
Epoch: 4, Step: 04800/8865, Loss: 0.6328
Epoch: 4, Step: 04900/8865, Loss: 0.6331
Epoch: 4, Step: 05000/8865, Loss: 0.6332
Epoch: 4, Step: 05100/8865, Loss: 0.6320
Epoch: 4, Step: 05200/8865, Loss: 0.6329
Epoch: 4, Step: 05300/8865, Loss: 0.6324
Epoch: 4, Step: 05400/8865, Loss: 0.6313
Epoch: 4, Step: 05500/8865, Loss: 0.6316
Epoch: 4, Step: 05600/8865, Loss: 0.6322
Epoch: 4, Step: 05700/8865, Loss: 0.6315
Epoch: 4, Step: 05800/8865, Loss: 0.6328
Epoch: 4, Step: 05900/8865, Loss: 0.6322
Epoch: 4, Step: 06000/8865, Loss: 0.6320
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 4, Step: 06000/8865, Acc: 0.3087
Epoch: 4, Step: 06100/8865, Loss: 0.6314
Epoch: 4, Step: 06200/8865, Loss: 0.6315
Epoch: 4, Step: 06300/8865, Loss: 0.6314
Epoch: 4, Step: 06400/8865, Loss: 0.6318
Epoch: 4, Step: 06500/8865, Loss: 0.6313
Epoch: 4, Step: 06600/8865, Loss: 0.6310
Epoch: 4, Step: 06700/8865, Loss: 0.6308
Epoch: 4, Step: 06800/8865, Loss: 0.6299
Epoch: 4, Step: 06900/8865, Loss: 0.6307
Epoch: 4, Step: 07000/8865, Loss: 0.6304
Epoch: 4, Step: 07100/8865, Loss: 0.6306
Epoch: 4, Step: 07200/8865, Loss: 0.6307
Epoch: 4, Step: 07300/8865, Loss: 0.6303
Epoch: 4, Step: 07400/8865, Loss: 0.6304
Epoch: 4, Step: 07500/8865, Loss: 0.6299
Epoch: 4, Step: 07600/8865, Loss: 0.6300
Epoch: 4, Step: 07700/8865, Loss: 0.6300
Epoch: 4, Step: 07800/8865, Loss: 0.6298
Epoch: 4, Step: 07900/8865, Loss: 0.6298
Epoch: 4, Step: 08000/8865, Loss: 0.6298
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 4, Step: 08000/8865, Acc: 0.3117
Epoch: 4, Step: 08100/8865, Loss: 0.6289
Epoch: 4, Step: 08200/8865, Loss: 0.6298
Epoch: 4, Step: 08300/8865, Loss: 0.6289
Epoch: 4, Step: 08400/8865, Loss: 0.6294
Epoch: 4, Step: 08500/8865, Loss: 0.6288
Epoch: 4, Step: 08600/8865, Loss: 0.6295
Epoch: 4, Step: 08700/8865, Loss: 0.6299
Epoch: 4, Step: 08800/8865, Loss: 0.6294
z device: cuda:0
train_idx device: cuda:0, test_idx device: cuda:0
y device: cuda:0
Epoch: 4, Accuracy: 0.3100
Fetching embeddings for node type: author
Fetching embeddings for node type: field_of_study
Fetching embeddings for node type: institution
Fetching embeddings for node type: paper
Embeddings gemt i mappen: embeddings/lr_0.01_dim_128_walklen_64_walks_5
